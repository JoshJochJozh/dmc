{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are not fundamentally economic. the anti-immigrant, anti-mexican, anti-muslim and anti-refugee senti --> m\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 62s - loss: 3.2025 - val_loss: 2.9716\n",
      "----- generating with seed: took home about 20- to 30-times as much as their average worker. the reduction or elimination of thi\n",
      "----- diversity: 0.5\n",
      "took home about 20- to 30-times as much as their average worker. the reduction or elimination of thith sdiaoe dni n  seai l a  lt t t  aoehii e tisnneeaa  n e  r d ecttotlsa   n  ti eecn iat ga r  i t\n",
      "----- diversity: 1.2\n",
      "took home about 20- to 30-times as much as their average worker. the reduction or elimination of thizeru- ddsckm yyfar8iei.cs odht.mnmcftfetsaggw 9msn b\n",
      "vnxuvneoaareiowdorir ec alhma c i,ponucisrtntnk\n",
      "epoch: 2 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 3.0244 - val_loss: 2.9309\n",
      "----- generating with seed: ore durable, growing economy; 15m new private-sector jobs since early 2010; rising wages, falling po\n",
      "----- diversity: 0.5\n",
      "ore durable, growing economy; 15m new private-sector jobs since early 2010; rising wages, falling pon e eoo a oaaaotet on   h   eesapnpgt e nefreaih aenet o eeehoe ts atneao  nft  eiaforeo ie olaeeeae\n",
      "----- diversity: 1.2\n",
      "ore durable, growing economy; 15m new private-sector jobs since early 2010; rising wages, falling poiaso  tmcen99uia tiss,ip eiviedsotcieeuptsuyoltc  tiiq7 naeorulitfihle9kn?yo o8g s rtrcv-vroude? shv\n",
      "epoch: 3 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 68s - loss: 2.9669 - val_loss: 2.8716\n",
      "----- generating with seed: ity, ensuring that everyone who wants a job can get one and building a resilient economy thats prime\n",
      "----- diversity: 0.5\n",
      "ity, ensuring that everyone who wants a job can get one and building a resilient economy thats prime   es o  aoeene o l ieeso o aoo  nnree eo sa arieleri ai  retind elen   enthod   vere rseoo he ststw\n",
      "----- diversity: 1.2\n",
      "ity, ensuring that everyone who wants a job can get one and building a resilient economy thats prime  itrait3nv ;oahln n apu emnwt ndc r er- natlsoe m hvlarge ua mo ntactgod tmldipaot focx bliot5ptppn\n",
      "epoch: 4 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 69s - loss: 2.8895 - val_loss: 2.7730\n",
      "----- generating with seed: itedperhaps more than any otherfrom immigration, trade and technological innovation suddenly develop\n",
      "----- diversity: 0.5\n",
      "itedperhaps more than any otherfrom immigration, trade and technological innovation suddenly develops ai e it e ete iha n poehe  atlre e ni nre et cit t tre te edt  earo  oalr td io nmm atrne se amis \n",
      "----- diversity: 1.2\n",
      "itedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developartt etacgyeerd t3?tthn, ynsa e eupnor m agyam, w suirc5ieh grle \n",
      "aduska iynterdrlmtn lrre ves rncie\n",
      "epoch: 5 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 70s - loss: 2.7765 - val_loss: 2.6515\n",
      "----- generating with seed: e participation has fallen most precipitously.\n",
      "\n",
      "there are many ways to keep more americans in the la\n",
      "----- diversity: 0.5\n",
      "e participation has fallen most precipitously.\n",
      "\n",
      "there are many ways to keep more americans in the lan as rhesea rth, s erte etvor f the art a n fis aisc onse erees thein an ghecal sere to t the bae th\n",
      "----- diversity: 1.2\n",
      "e participation has fallen most precipitously.\n",
      "\n",
      "there are many ways to keep more americans in the la ,y\n",
      "iond,lmnsi0te annreg ks eeuaelfr etoirh ehnalcgioc, ,z-i sdltion f vek,mo-vspehd w n7 ity;aho bo\n",
      "epoch: 6 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 82s - loss: 2.6650 - val_loss: 2.5479\n",
      "----- generating with seed: politics. my administration secured much more fiscal expansion than many appreciated in recovering f\n",
      "----- diversity: 0.5\n",
      "politics. my administration secured much more fiscal expansion than many appreciated in recovering fs tha d oorreit th ann ale rem canyrt oan as tha aal  oo eceis the ane icare cath il eaes orsis eld \n",
      "----- diversity: 1.2\n",
      "politics. my administration secured much more fiscal expansion than many appreciated in recovering fr ;arktiwi.  ahaale m-opreo ett aco  ol euranerggs -priree vote,r sreiisb init typ\n",
      "tendmina iutbnica\n",
      "epoch: 7 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 75s - loss: 2.5707 - val_loss: 2.4835\n",
      "----- generating with seed: ome growth for low- and middle-income families. globalisation and automation have weakened the posit\n",
      "----- diversity: 0.5\n",
      "ome growth for low- and middle-income families. globalisation and automation have weakened the posite in se ant ares on d tort an the tale the th fhe the inn re ins acinnt an ateire set potob the wili\n",
      "----- diversity: 1.2\n",
      "ome growth for low- and middle-income families. globalisation and automation have weakened the positara,dn ri-womq caoss orue ucbndg, xo aub-edatmi necw2n teelyiuale cl cape pribifit alo ind pftneween\n",
      "epoch: 8 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 75s - loss: 2.5240 - val_loss: 2.4355\n",
      "----- generating with seed: y are receptive to the argument that the game is rigged. but amid this understandable frustration, m\n",
      "----- diversity: 0.5\n",
      "y are receptive to the argument that the game is rigged. but amid this understandable frustration, me the anas rort an tind the the the te the gorith  ouret are on the than sher ane to arecinceod the \n",
      "----- diversity: 1.2\n",
      "y are receptive to the argument that the game is rigged. but amid this understandable frustration, maveogugss tauglagpataheg. ano, epgeoln tna o adivestho orgeibuy te gupla9ng iand\n",
      "ccineg tabd fie3a.s\n",
      "epoch: 9 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 75s - loss: 2.4778 - val_loss: 2.4004\n",
      "----- generating with seed: ues have also played a major role. in the past, differences in pay between corporate executives and \n",
      "----- diversity: 0.5\n",
      "ues have also played a major role. in the past, differences in pay between corporate executives and an and tioe onis the ther than so pore am the aning are than on ansirare on sumilin ane tor ast an e\n",
      "----- diversity: 1.2\n",
      "ues have also played a major role. in the past, differences in pay between corporate executives and carect i.o eriulyiy woadt enafgty,omat gpoy aecalc iodeas. rapvaas edubred\n",
      "sian,i anghaslinsrert, gc\n",
      "epoch: 10 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 76s - loss: 2.4395 - val_loss: 2.3706\n",
      "----- generating with seed: oblem with increased inequalityit diminishes upward mobility. it makes the top and bottom rungs of t\n",
      "----- diversity: 0.5\n",
      "oblem with increased inequalityit diminishes upward mobility. it makes the top and bottom rungs of the suon ind were wor the for or an ine the the ande on to ae th int the the sorere cand on wors ices\n",
      "----- diversity: 1.2\n",
      "oblem with increased inequalityit diminishes upward mobility. it makes the top and bottom rungs of touthe gpanprive kancciwer ut erru ked batlliin apas-hloruterl-unt potprekcea ehe sy sef celiro ars\n",
      "y\n",
      "epoch: 11 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 76s - loss: 2.4095 - val_loss: 2.3425\n",
      "----- generating with seed: per nearby, technology allows anyone with a smartphone to see how the most privileged live. expectat\n",
      "----- diversity: 0.5\n",
      "per nearby, technology allows anyone with a smartphone to see how the most privileged live. expectat and ratican ins fale the thes the the pare be fren are in he sede de the palne tor the the poretin \n",
      "----- diversity: 1.2\n",
      "per nearby, technology allows anyone with a smartphone to see how the most privileged live. expectat ovis. ve cusdowe aod fhlle are colmaley xf re-fis7hangi.evitsen toty prn.keg gpollrsobo 2es whetsus\n",
      "epoch: 12 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 77s - loss: 2.3827 - val_loss: 2.3184\n",
      "----- generating with seed:  particularly for infrastructure; and a political system so partisan that previously bipartisan idea\n",
      "----- diversity: 0.5\n",
      " particularly for infrastructure; and a political system so partisan that previously bipartisan ideate somte than are decebes are con rles on and of the the mon the borithe ind an fite the ling and ar\n",
      "----- diversity: 1.2\n",
      " particularly for infrastructure; and a political system so partisan that previously bipartisan ideanr tos  centr-aclg ameneltd.pjudiy ir pyothewdy dithe spfdevobe yeloncmtunt rued col; geromee. thk u\n",
      "epoch: 13 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 77s - loss: 2.3462 - val_loss: 2.2969\n",
      "----- generating with seed:  are critical both for increasing economic growth and for ensuring that it is shared broadly. these \n",
      "----- diversity: 0.5\n",
      " are critical both for increasing economic growth and for ensuring that it is shared broadly. these tho the porewe sthe on feconte the the the the protitical congere ind anominn fins the bores mond th\n",
      "----- diversity: 1.2\n",
      " are critical both for increasing economic growth and for ensuring that it is shared broadly. these ilts cure.\n",
      " al deniliss the fmrcasbyo lore mnp geon. wallneves,k ts wheb tratiem bos tupnqasorecs 3p\n",
      "epoch: 14 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.3256 - val_loss: 2.2727\n",
      "----- generating with seed: t. research shows that growth is more fragile and recessions more frequent in countries with greater\n",
      "----- diversity: 0.5\n",
      "t. research shows that growth is more fragile and recessions more frequent in countries with greater an ore anticanor an of the wore and bices and to the that and ande the wand to al arles she protint\n",
      "----- diversity: 1.2\n",
      "t. research shows that growth is more fragile and recessions more frequent in countries with greater ine iassutig tha cronen, inigrceslsros ghof dveabio- aad .umcold sadm?alsoun bcc1we vins oup fale o\n",
      "epoch: 15 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.3007 - val_loss: 2.2593\n",
      "----- generating with seed: ight direction too.\n",
      "\n",
      "\n",
      "third, a successful economy also depends on meaningful opportunities for work \n",
      "----- diversity: 0.5\n",
      "ight direction too.\n",
      "\n",
      "\n",
      "third, a successful economy also depends on meaningful opportunities for work baliticas yhear in arather fing are for an the the the the as an coreien en the the thac ante mouler\n",
      "----- diversity: 1.2\n",
      "ight direction too.\n",
      "\n",
      "\n",
      "third, a successful economy also depends on meaningful opportunities for work wbeilnson owem ntir btingr1-pipenian fqu1nc-dbout what oxpbncod alin sucr;w9 rh ass awlcyk-taq avle \n",
      "epoch: 16 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 74s - loss: 2.2731 - val_loss: 2.2418\n",
      "----- generating with seed: ierharder to move up and harder to lose your place at the top.\n",
      "\n",
      "economists have listed many causes f\n",
      "----- diversity: 0.5\n",
      "ierharder to move up and harder to lose your place at the top.\n",
      "\n",
      "economists have listed many causes fal wer tha than was buse on teat pariting ronte chart an be the and peow the an bowig ing an the to \n",
      "----- diversity: 1.2\n",
      "ierharder to move up and harder to lose your place at the top.\n",
      "\n",
      "economists have listed many causes fg gurkinnpe..\n",
      "\n",
      "ucsnor.to\n",
      "\n",
      "ss attd sop 1he thay b1?k w2:h eesud eunslsticinnerine, th sor.s. ?votomc \n",
      "epoch: 17 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 74s - loss: 2.2583 - val_loss: 2.2240\n",
      "----- generating with seed: american financial institutions no longer get the type of easier funding they got beforeevidence tha\n",
      "----- diversity: 0.5\n",
      "american financial institutions no longer get the type of easier funding they got beforeevidence that and to thop tore tho perent ian thal inons tad ial wewit al intor the the pallit on the pootes os \n",
      "----- diversity: 1.2\n",
      "american financial institutions no longer get the type of easier funding they got beforeevidence thak und cepe,resof rlose acol t, ur gare wpysing0\n",
      " aare hho. pand ou chomass, sen 3ebesto seanlt e vew\n",
      "epoch: 18 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 74s - loss: 2.2384 - val_loss: 2.2167\n",
      "----- generating with seed: ology that rejects virtually all sources of new public funding; a fixation on deficits at the expens\n",
      "----- diversity: 0.5\n",
      "ology that rejects virtually all sources of new public funding; a fixation on deficits at the expensims fariche the and and in were the can ind portice on wheg. bat ane nant and and the and or art ine\n",
      "----- diversity: 1.2\n",
      "ology that rejects virtually all sources of new public funding; a fixation on deficits at the expenspoices ednurew er bye. ery alisisceseys,r th? s-paup ande aus asd reskly foat the woplito;\n",
      " bmsy 3 d\n",
      "epoch: 19 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 74s - loss: 2.2186 - val_loss: 2.1974\n",
      "----- generating with seed:  the wage gains people want, regardless of how we divide up the pie.\n",
      "\n",
      "a major source of the recent p\n",
      "----- diversity: 0.5\n",
      " the wage gains people want, regardless of how we divide up the pie.\n",
      "\n",
      "a major source of the recent ping buticen wthe the f prore ant pondens the the proesing in antiont lomiling that bere the sores of\n",
      "----- diversity: 1.2\n",
      " the wage gains people want, regardless of how we divide up the pie.\n",
      "\n",
      "a major source of the recent pcanoadrish onm, kecinsstiveyorits th out, aly rate. wicniged cuigred pat depnlthes mawecsmermite ha \n",
      "epoch: 20 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.1952 - val_loss: 2.1849\n",
      "----- generating with seed: owth, including more capital for american banks, less reliance on short-term funding, and better ove\n",
      "----- diversity: 0.5\n",
      "owth, including more capital for american banks, less reliance on short-term funding, and better ove the apdes moor the the asterans and fithe anting ardesting out the and pus werica than the seste th\n",
      "----- diversity: 1.2\n",
      "owth, including more capital for american banks, less reliance on short-term funding, and better ove s \n",
      "enpecride tnet alt rsucomecang teat eneurinss an ix nagtid..; ho wtor io1m rareedsanchotdeaas mg\n",
      "epoch: 21 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.1779 - val_loss: 2.1831\n",
      "----- generating with seed: lege-educated americansthe group where labour-force participation has fallen most precipitously.\n",
      "\n",
      "th\n",
      "----- diversity: 0.5\n",
      "lege-educated americansthe group where labour-force participation has fallen most precipitously.\n",
      "\n",
      "the bret tad the ond the ressons that parle thes seromer and borevinghis the passing on ald and more b\n",
      "----- diversity: 1.2\n",
      "lege-educated americansthe group where labour-force participation has fallen most precipitously.\n",
      "\n",
      "th abthain wod collivery andet.t alm reyhar nsnges couddcand rmeicchamiksigng the forquaysty thocp mte\n",
      "epoch: 22 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 75s - loss: 2.1623 - val_loss: 2.1673\n",
      "----- generating with seed: s cycle than in any since the 1970s. these gains would have been impossible without the globalisatio\n",
      "----- diversity: 0.5\n",
      "s cycle than in any since the 1970s. these gains would have been impossible without the globalisation that eannticing intint incels fore cols mithe the sorte the the and inscale tas for and and cander\n",
      "----- diversity: 1.2\n",
      "s cycle than in any since the 1970s. these gains would have been impossible without the globalisation ramy-kirwttatimrsame cos e3fotrtst-7bing g?eshind imabling enothe tt actimuins tecerone mo, awnge \n",
      "epoch: 23 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.1372 - val_loss: 2.1538\n",
      "----- generating with seed: church, at their childrens schools, in civic organisations. thats why ceos took home about 20- to 30\n",
      "----- diversity: 0.5\n",
      "church, at their childrens schools, in civic organisations. thats why ceos took home about 20- to 30 the presencing hat the and of the bating to the the presting bat in the thas the great thes the rea\n",
      "----- diversity: 1.2\n",
      "church, at their childrens schools, in civic organisations. thats why ceos took home about 20- to 30 kinx tieuriwt t efureceecannice arris mat miiges ctadeiss tomaby laove fo toud murts, akdxi8stiyd t\n",
      "epoch: 24 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.1204 - val_loss: 2.1498\n",
      "----- generating with seed:  a critical role. they help workers get a bigger slice of the pie but they need to be flexible enoug\n",
      "----- diversity: 0.5\n",
      " a critical role. they help workers get a bigger slice of the pie but they need to be flexible enouge the wore cofore sure to on ally anteme orserens wore and the andere the buat be the worke sadeting\n",
      "----- diversity: 1.2\n",
      " a critical role. they help workers get a bigger slice of the pie but they need to be flexible enoug fto pubeisise jucan pembat:s loferre. ty baiciteny omatddecaly 1iseveno. t oan aorrinwmyciinocnlo c\n",
      "epoch: 25 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.1071 - val_loss: 2.1344\n",
      "----- generating with seed: equires recognising that americas economy is an enormously complicated mechanism. as appealing as so\n",
      "----- diversity: 0.5\n",
      "equires recognising that americas economy is an enormously complicated mechanism. as appealing as so bell the fatinits ins in furtord and sicin all and re bety for the that prasting the fahe the the f\n",
      "----- diversity: 1.2\n",
      "equires recognising that americas economy is an enormously complicated mechanism. as appealing as sogreg th thow fon khaster-moriten the angonaesing thi\n",
      "s ruli aling th eospetle hoos haped. inseticiin\n",
      "epoch: 26 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 74s - loss: 2.0928 - val_loss: 2.1344\n",
      "----- generating with seed: ives market economies.\n",
      "\n",
      "america has shown that progress is possible. last year, income gains were la\n",
      "----- diversity: 0.5\n",
      "ives market economies.\n",
      "\n",
      "america has shown that progress is possible. last year, income gains were last. buthe to the for candenincat an the andest an in of ande tice ant and comortent and wy les pore \n",
      "----- diversity: 1.2\n",
      "ives market economies.\n",
      "\n",
      "america has shown that progress is possible. last year, income gains were la tore iras andartien inw putting jork for7 ababisysi. fouc mowt andsiig co hiseuadd cinem, alles, as\n",
      "epoch: 27 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.0715 - val_loss: 2.1140\n",
      "----- generating with seed: eliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress and peril has \n",
      "----- diversity: 0.5\n",
      "eliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress and peril has reaple bos were ande that on the sessime shating and be meeing the bion the fat has for mere al sest\n",
      "----- diversity: 1.2\n",
      "eliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress and peril has the fonarla whono rettadisy efolmas saloprems lod. thet erasming erpopty oi4m thoveo tes gloss, the \n",
      "epoch: 28 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.0566 - val_loss: 2.1036\n",
      "----- generating with seed: king the global economy work better for all people, not just those at the top.\n",
      "\n",
      "a force for good\n",
      "\n",
      "th\n",
      "----- diversity: 0.5\n",
      "king the global economy work better for all people, not just those at the top.\n",
      "\n",
      "a force for good\n",
      "\n",
      "the sesting the be the thes the seprecing for and the the sesticins workers proded anis mering alo at \n",
      "----- diversity: 1.2\n",
      "king the global economy work better for all people, not just those at the top.\n",
      "\n",
      "a force for good\n",
      "\n",
      "the trouss iul o, khic, ory ahr fixingeverectatiprgattoms wof orss the aldane toiso cpviscjfon 4feteab\n",
      "epoch: 29 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.0429 - val_loss: 2.1112\n",
      "----- generating with seed: ack together again without real consequences for real people.\n",
      "\n",
      "instead, fully restoring faith in an \n",
      "----- diversity: 0.5\n",
      "ack together again without real consequences for real people.\n",
      "\n",
      "instead, fully restoring faith in an the progut in thes thes and the tist to the cerese in wher that nour that ald tovans and enoromiss o\n",
      "----- diversity: 1.2\n",
      "ack together again without real consequences for real people.\n",
      "\n",
      "instead, fully restoring faith in an leodubmifitilisat if eplrlies pd-syyrescn wupf.ity consemot onvanblaed. yeass. bbes to ane to ardanc\n",
      "epoch: 30 / 30\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 73s - loss: 2.0217 - val_loss: 2.0956\n",
      "----- generating with seed:  all the work that remains, a new foundation is laid. a new future is ours to write. it must be one \n",
      "----- diversity: 0.5\n",
      " all the work that remains, a new foundation is laid. a new future is ours to write. it must be one second the and lobe thes the thes serser an in comersane for the ardering ars bating the thes and an\n",
      "----- diversity: 1.2\n",
      " all the work that remains, a new foundation is laid. a new future is ours to write. it must be one 2wplt3 elsidicg it icaf the the anfutlibies wlhems nostitndon w-17bl. vewele alt athe fnyrs anduricy\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
